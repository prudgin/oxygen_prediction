{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "noteable": {
      "last_transaction_id": "ede7920f-93ff-4aa1-b988-5523be4025e2"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "17582059-c446-470b-ac93-178d1c185b1b",
      "cell_type": "markdown",
      "source": "## Pond dissolved oxygen prediction\n\nThis notebook starts a series of efforts to predict DO in aquaculture ponds.\n\nHere, the goal is to predict DO utilizing solely DO data. Measurements were taken twice daily, during the morning and evening, aligning closely with sunrise and sunset timings (with a variance of +/- one hour).\n\nTLDR: The model accurately captures the overarching trends, presenting an error (RMSE) ranging from 0.8 to 2. It performs well, especially when the data remains relatively stable. However, during episodes of heightened DO fluctuations, particularly those caused by liming or the introduction of manure (which exerts a substantial oxygen demand), the model struggles to adapt.\n\nScroll to the bottom to see predicted vs real data graphs.\n\nA SARIMA model will be used. SARIMA, or Seasonal Autoregressive Integrated Moving Average, is a forecasting method that analyzes time-based data by considering its past values (autoregressive), handling trends and irregularities (integrated), and factoring in past forecast errors (moving average). It extends its capabilities by accounting for repeating patterns over specific intervals, like seasons. By combining these components, SARIMA can provide accurate predictions for future data points, making it a valuable tool for understanding and forecasting various cyclic phenomena.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "ecc3ac2f",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "output_collection_id": "91d7cccf-6e40-4cf4-9d76-263d77400468"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:09:07.831715+00:00",
          "start_time": "2023-08-27T17:09:07.655877+00:00"
        },
        "jupyter": {
          "source_hidden": false
        }
      },
      "execution_count": null,
      "source": "import pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom scipy.stats import boxcox\nfrom scipy.special import inv_boxcox\nimport matplotlib.pyplot as plt\n\n# Load the data. One pd series with DO values (o2) for each pond.\npond_names = ['np1', 'np2', 'vp1', 'vp2', 'vp3', 'vp4']\no2_series = {}\nprint('Loading data. One pd series with DO values (o2) for each pond.')\nfor pond_name in pond_names:\n    o2_series[pond_name] = pd.read_csv('o2_only_ponds/'+pond_name+'_series.csv')['o2']\n    print(f'pond: {pond_name}, entries: {len(o2_series[pond_name])}')",
      "outputs": []
    },
    {
      "id": "439b8f9d-5330-474b-8187-baefbf512fb9",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "face20e3-e116-41d2-9544-645b102202f4"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:09:09.650606+00:00",
          "start_time": "2023-08-27T17:09:07.841944+00:00"
        }
      },
      "execution_count": null,
      "source": "# Creating a 2x3 grid for the ponds (2 rows, 3 columns)\nfig, axes = plt.subplots(3, 2, figsize=(15, 10))\nfig.suptitle('O2 Values for Different Ponds')\n\n# Flatten the axes array for easier indexing\naxes = axes.ravel()\n\nfor idx, pond_name in enumerate(pond_names):\n    # Plotting the O2 values for each pond\n    axes[idx].plot(o2_series[pond_name], label=f'{pond_name} O2 Values')\n    axes[idx].set_title(pond_name)\n    axes[idx].set_xlabel('Entry index')\n    axes[idx].set_ylabel('O2 Value')\n    axes[idx].legend()\n    axes[idx].grid(True, linestyle='--', alpha=0.5)\n\n# Adjust the layout so that plots do not overlap\nplt.tight_layout()\nplt.subplots_adjust(top=0.90)  # Adjust the top spacing so that the main title doesn't overlap\nplt.show()\n",
      "outputs": []
    },
    {
      "id": "a96c976b-2a3a-47ab-80c2-a85433ca298a",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "3fc55213-f53d-4f72-a936-a14e61bc67a3"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:09:10.080641+00:00",
          "start_time": "2023-08-27T17:09:09.920251+00:00"
        }
      },
      "execution_count": null,
      "source": "for pond_name in o2_series.keys():\n    ser = o2_series[pond_name]\n    print(f'pond: {pond_name}, mean: {ser.mean():.2f}, '\n          f'std: {ser.std():.2f}, std/mean: {ser.std()/ser.mean():.2f}')",
      "outputs": []
    },
    {
      "id": "8e3c4939-2d54-40bf-b2b9-2d14d2afca14",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "e47b142e-5ced-405f-a079-99b11984b759"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:09:11.988361+00:00",
          "start_time": "2023-08-27T17:09:10.090596+00:00"
        }
      },
      "execution_count": null,
      "source": "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\nfig.suptitle('O2 with substracted mean')\naxes = axes.ravel()\nfor idx, pond_name in enumerate(pond_names):\n    axes[idx].hist(o2_series[pond_name]-o2_series[pond_name].mean(), bins = 15)\n    axes[idx].set_title(pond_name)\n    axes[idx].set_xlabel('O2')\n    axes[idx].set_ylabel('Number of entries')\n    axes[idx].legend()\n    axes[idx].grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.subplots_adjust(top=0.90)\nplt.show()",
      "outputs": []
    },
    {
      "id": "6668ea59-ee3f-4042-8699-e4aad16c5bd6",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "18e0ed8b-4729-4653-a30b-f138d2822a28"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:09:12.380820+00:00",
          "start_time": "2023-08-27T17:09:12.210492+00:00"
        }
      },
      "execution_count": null,
      "source": "for pond_name in o2_series.keys():\n    ser = o2_series[pond_name]\n    jb_test = sm.stats.stattools.jarque_bera(ser)\n    print(f'pond: {pond_name}, JB: {jb_test[0]:.1f}, p-value: {jb_test[1]:.2f},'\n          f'skew: {jb_test[2]:.1f}, kurtosis: {jb_test[3]:.1f}')",
      "outputs": []
    },
    {
      "id": "9ecf1d03-80d6-4437-9094-ba0a38a64f99",
      "cell_type": "markdown",
      "source": "### Data normality analysis\n\nWe can see that O2 data for ponds np2 and vp1 is normally distributed, while for the others it is not.\n\nFor each pond, we calculated the following metrics related to oxygen (O2) concentration:\n\nMean (central tendency)\nStandard deviation (variability)\nRatio of standard deviation to mean (relative variability)\nThese statistics give us insights into the general behavior and variability of oxygen concentrations in each pond. A higher standard deviation to mean ratio indicates higher relative variability.\n\nJarque-Bera Test for Normality:\n\nThe Jarque-Bera test is a statistical test that checks if the data has the skewness and kurtosis of a normal distribution. The test provides a JB statistic, p-value, skewness, and kurtosis value for each dataset.\n\nJB Statistic: Higher values indicate a departure from normality.\np-value: A smaller p-value (typically < 0.05) suggests that the data is not normally distributed.\nSkewness: Reflects the data's symmetry. A value of zero means the data is perfectly symmetrical, while positive/negative values indicate skewed distributions.\nKurtosis: Measures the \"tailedness\" of the distribution. A kurtosis > 3 indicates a leptokurtic distribution (heavy-tailed), and < 3 indicates a platykurtic distribution (light-tailed).\nFor SARIMA modeling, the assumption is that the residuals (or errors) from the model are normally distributed. The Jarque-Bera test helps verify this assumption.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "d184081f-8308-43a7-9954-150ba4d26ca6",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "dfb7f57f-141e-4bf5-b473-5e83ab2c8b8e"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:09:12.610394+00:00",
          "start_time": "2023-08-27T17:09:12.390820+00:00"
        }
      },
      "execution_count": null,
      "source": "for pond_name in o2_series.keys():\n    ser = o2_series[pond_name]\n    \n    test = sm.tsa.adfuller(ser)\n    critical_values = {key: f\"{value:.1f}\" for key, value in test[4].items()}\n    print(f'{pond_name}: adf {test[0]:.1f}, p-value {test[1]:.2f}, '\n          f'Critical values {critical_values}')\n    \n    if test[0]> test[4]['5%']: \n        print('unit roots, unstationary')\n    else:\n        print('no unit roots, stationary')",
      "outputs": []
    },
    {
      "id": "90d5d970-5efa-4366-b211-ed3ddbbe7917",
      "cell_type": "markdown",
      "source": "### Stationarity\n\nIn time series analysis, stationarity is a fundamental concept. A time series is said to be stationary if its statistical properties, such as mean, variance, and autocorrelation, are all constant over time. In simpler terms, if you take any segment of the series, its characteristics would be similar to any other segment. This is essential because most time series forecasting methods, including SARIMA, assume or require the series to be stationary.\n\nA non-stationary series, on the other hand, shows patterns, trends, or seasonality. This means that over time, its mean, variance, or behavior might change.\n\nADF Test\n\nThe Augmented Dickey-Fuller (ADF) test is a popular method to test the stationarity of a series. The core idea behind the test is to determine if there's a unit root in the series. If present, it suggests the series is non-stationary. The test returns several statistics, including:\n\nadf - The test statistic. A more negative value indicates stronger evidence against the hypothesis of a unit root.\np-value - This shows the probability of observing the test result if the series has a unit root. A small p-value (typically â‰¤ 0.05) indicates that the series is likely stationary.\nCritical values - These provide thresholds at which we can decide whether or not to reject the hypothesis of a unit root.\nInterpretations\n\nnp1: The ADF test statistic (-3.4) is more negative than the 5% critical value (-2.9) and the p-value is 0.01, which is below the common alpha level of 0.05. This indicates strong evidence against the unit root hypothesis, suggesting that the series is stationary.\n\nnp2: Similar to np1, the test statistic (-3.0) suggests the series is stationary.\n\nvp1: With an ADF statistic of -2.1, which is less negative than the 5% critical value, and a p-value of 0.23, this series appears to have a unit root and is non-stationary.\n\nvp2 & vp3: Both ponds have ADF statistics much more negative than the 5% threshold, indicating stationarity.\n\nvp4: The ADF statistic (-2.7) is closer to the 5% threshold and the p-value is slightly above 0.05. This means there's weaker evidence against the unit root hypothesis, suggesting potential non-stationarity in the series.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "b0942c78-e28f-4c63-9e4f-3eea7807cc0c",
      "cell_type": "markdown",
      "source": "### Data transformation\n\nAs we have seen, the data have issues with normality and stationarity. Let's addres normality first.\n\n#### Box-Cox Transformation\n\nThe Box-Cox transformation is a family of power transformations that are used to stabilize variance and make a dataset more closely follow a normal distribution. It's particularly useful in time series analysis where the stabilization of variance can lead to better forecasting performance.\n\nLet's perform it on the data!",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "76730c14-398d-4b51-b11f-9d7db2b749cc",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "python",
          "output_collection_id": "612eac9d-525a-4191-a8fb-745ede3f6676"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:09:12.910714+00:00",
          "start_time": "2023-08-27T17:09:12.737632+00:00"
        },
        "jupyter": {
          "source_hidden": true
        }
      },
      "execution_count": null,
      "source": "# Box-Cox transformation\no2_boxcox = {}\nfor pond_name in o2_series.keys():\n    ser = o2_series[pond_name]\n    sert, lambda_best_fit = boxcox(ser)\n    sert = pd.Series(sert, name='o2')\n    print(f'Optimal lambda value for pond {pond_name}: {lambda_best_fit:.2f}')\n    o2_boxcox[pond_name] = {'data':sert, 'lambda':lambda_best_fit}\n",
      "outputs": []
    },
    {
      "id": "130cf82b-ac78-4771-b888-85df47b3b099",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "f7724661-0f13-492c-b2b0-249abdb5ea77"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:09:13.129560+00:00",
          "start_time": "2023-08-27T17:09:12.939686+00:00"
        }
      },
      "execution_count": null,
      "source": "for pond_name in o2_series.keys():\n    ser = o2_series[pond_name]\n    jb_test = sm.stats.stattools.jarque_bera(ser)\n    print(f'before: pond: {pond_name}, JB: {jb_test[0]:.1f}, p-value: {jb_test[1]:.2f},'\n          f'skew: {jb_test[2]:.1f}, kurtosis: {jb_test[3]:.1f}')\n    \n    sert = o2_boxcox[pond_name]['data']\n    jb_test = sm.stats.stattools.jarque_bera(sert)\n    print(f'after: pond: {pond_name}, JB: {jb_test[0]:.1f}, p-value: {jb_test[1]:.2f},'\n          f'skew: {jb_test[2]:.1f}, kurtosis: {jb_test[3]:.1f}\\n')\n    ",
      "outputs": []
    },
    {
      "id": "4e1eb418-90a5-4112-bee0-4a0a2f9f9283",
      "cell_type": "markdown",
      "source": "### Effects of the Box-Cox Transformation\n##### np1 Pond:\nBefore Transformation: The Jarque-Bera test statistic was 21.7 with a p-value of 0.00, indicating significant deviation from normality. The skewness of 0.8 and kurtosis of 3.9 further confirmed this.\nAfter Transformation: The test statistic dropped to 2.9, and the p-value increased to 0.24, suggesting a closer fit to normality. The skewness became almost 0, which is a good indication of symmetry.\n\n##### np2 Pond:\nBefore: The dataset seemed relatively normal with a JB test statistic of 1.2 and a p-value of 0.55.\nAfter: Even further improvements were seen post-transformation with a decreased JB statistic of 0.6 and increased p-value of 0.73.\n\n##### vp1 Pond:\nBefore: With a JB statistic of 2.1 and p-value of 0.35, this pond's data was not too far from normality.\nAfter: The transformation slightly improved the normality with a test statistic of 0.5 and a p-value of 0.77.\n\n##### vp2 Pond:\nBefore: The JB test indicated a deviation from normality with a statistic of 10.4 and p-value of 0.01.\nAfter: Post-transformation, the results were more favorable with a JB statistic of 1.4 and p-value of 0.51.\n\n##### vp3 Pond:\nBefore: Significant deviation from normality was evident with a JB test statistic of 19.9 and a p-value close to 0.\nAfter: While the transformation improved the distribution to a JB statistic of 7.8, the p-value remained significant at 0.02.\n\n##### vp4 Pond:\nBefore: The data showed a clear deviation from normality with a JB statistic of 18.8.\nAfter: Despite applying the Box-Cox transformation, the data still showed some deviation from a normal distribution, with a JB statistic of 11.7, although it was reduced compared to the original.\n\nThe Box-Cox transformation appeared to improve the normality of most ponds' data, as evidenced by the Jarque-Bera test results. ",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "94647152-30ee-4e40-a62e-ddd6fe0a0601",
      "cell_type": "markdown",
      "source": "### Improving Stationarity\n\nOur dataset captures oxygen levels at two distinct times: in the morning and the evening. Due to the inherent nature of diurnal cycles, oxygen concentration tends to be higher in the evenings. To account for this diurnal seasonality and enhance stationarity, we'll employ a difference-based approach:\n\nBy taking the difference between each data point and its corresponding value from two periods (lags) prior, we're essentially subtracting evening data from subsequent evening readings and morning data from subsequent morning readings.\n\nThis method can help mitigate the effects of this seasonality and make the dataset more stationary, which is beneficial for time series analysis.\n\nThe next cell output will demonstrate this.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "912e7906-6b49-46ff-b33e-9400146f0933",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "python",
          "output_collection_id": "ca8eb940-dfce-4f02-acd2-8ef16ed17f8b"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:09:13.450502+00:00",
          "start_time": "2023-08-27T17:09:13.238898+00:00"
        },
        "jupyter": {
          "source_hidden": true
        }
      },
      "execution_count": null,
      "source": "for pond_name in o2_series.keys():\n    sert = o2_boxcox[pond_name]['data']\n    serts = (sert - sert.shift(2)).dropna()\n    test = sm.tsa.adfuller(serts)\n    critical_values = {key: f\"{value:.1f}\" for key, value in test[4].items()}\n    print(f'{pond_name}: adf {test[0]:.1f}, p-value {test[1]:.2f}, '\n          f'Critical values {critical_values}')\n    \n    if test[0]> test[4]['5%']: \n        print('unit roots, unstationary')\n    else:\n        print('no unit roots, stationary')\n",
      "outputs": []
    },
    {
      "id": "9db5b9a1-0091-4c87-b88a-c750464fd31b",
      "cell_type": "markdown",
      "source": "### Let's take a look at transformed data\n\nUpon applying the aforementioned transformation, we observe that the data now appears to fluctuate consistently around a central value, zero in this case. This is indicative of improved stationarity, wherein the series doesn't exhibit obvious trends or seasonality but instead oscillates around a central tendency. ",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "e58c06fd-83c4-4e8b-8427-78be89414fd1",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "python",
          "output_collection_id": "abac94b6-bb86-42d8-99af-19e815c0d659"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:09:15.431277+00:00",
          "start_time": "2023-08-27T17:09:13.463326+00:00"
        },
        "jupyter": {
          "source_hidden": true
        }
      },
      "execution_count": null,
      "source": "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\nfig.suptitle('Transformed O2 Values')\naxes = axes.ravel()\nfor idx, pond_name in enumerate(pond_names):\n    sert = o2_boxcox[pond_name]['data']\n    serts = (sert - sert.shift(2)).dropna()\n    axes[idx].plot(serts, label=f'{pond_name} O2 Values')\n    axes[idx].set_title(pond_name)\n    axes[idx].set_xlabel('Entry index')\n    axes[idx].set_ylabel('Transformed O2')\n    axes[idx].legend()\n    axes[idx].grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.subplots_adjust(top=0.90)\nplt.show()",
      "outputs": []
    },
    {
      "id": "c83a57dd-697b-4928-ab92-87663462c7ee",
      "cell_type": "markdown",
      "source": "### Analyzing Autocorrelation: ACF and PACF Plots\nBefore diving deeper into time series modeling, it's pivotal to understand the underlying autocorrelation structure of our dataset. To achieve this, we'll utilize the Autocorrelation Function (ACF) and the Partial Autocorrelation Function (PACF) plots.\n\nWhy ACF and PACF?\nAutocorrelation Function (ACF): This depicts the correlation of the time series with its own lags. It gives us an understanding of how an observation is related to its previous observations.\n\nPartial Autocorrelation Function (PACF): While ACF gives the total correlation, PACF represents the direct correlation between observations at two points in time, discounting the influence of all other observations.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "dff108d6-6dac-4542-a115-3171b4cae8bb",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "942bb614-024c-47e8-8c7f-fe87304157b9"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:09:20.371144+00:00",
          "start_time": "2023-08-27T17:09:15.939455+00:00"
        }
      },
      "execution_count": null,
      "source": "pond_names = list(o2_series.keys())\nn_ponds = len(pond_names)\n\n# Adjusting the vertical size based on number of ponds\nfig, axes = plt.subplots(n_ponds, 2, figsize=(15, 4*n_ponds))  \nfig.suptitle('Autocorrelation Analysis')\n\nfor idx, pond_name in enumerate(pond_names):\n    sert = o2_boxcox[pond_name]['data']\n    serts = (sert - sert.shift(2)).dropna()\n\n    # ACF plot for current pond\n    sm.graphics.tsa.plot_acf(serts, lags=25, ax=axes[idx][0])\n    axes[idx][0].set_title(f'ACF for {pond_name}')\n    axes[idx][0].grid(True, linestyle='--', alpha=0.5)\n    \n    # PACF plot for current pond\n    sm.graphics.tsa.plot_pacf(ser, lags=25, ax=axes[idx][1])\n    axes[idx][1].set_title(f'PACF for {pond_name}')\n    axes[idx][1].grid(True, linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.95, hspace=0.4)\nplt.show()",
      "outputs": []
    },
    {
      "id": "c104691f-2b2b-474c-af6e-17accebcdf13",
      "cell_type": "markdown",
      "source": "### SARIMA parameters selection\n\nACF and PACF graphs are used for SARIMA model parameters setting, more can be found here: https://arauto.readthedocs.io/en/latest/how_to_choose_terms.html\n\nI will use these parameters as starting points for auto_arima parameter search",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "ec2000a6-70ec-4669-a0ee-b2c72a39be24",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "72dec926-d297-4177-a8e3-71efcdfcfa1e"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:09:21.100357+00:00",
          "start_time": "2023-08-27T17:09:20.938360+00:00"
        }
      },
      "execution_count": null,
      "source": "sarima_params = {}\nsarima_params['np1'] = [(1,0,0), (1,1,1,2)]\nsarima_params['np2'] = [(1,0,0), (1,1,1,2)]\nsarima_params['vp1'] = [(1,0,0), (1,1,2,2)]\nsarima_params['vp2'] = [(1,0,0), (1,1,1,2)]\nsarima_params['vp3'] = [(3,0,1), (1,1,0,2)]\nsarima_params['vp4'] = [(1,0,1), (1,1,0,2)]\n",
      "outputs": []
    },
    {
      "id": "c9d1befc-0dc9-4c22-b80c-7d05d85bb573",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "33570aa8-9e2a-40c3-b637-5f2b630348aa"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:09:26.184095+00:00",
          "start_time": "2023-08-27T17:09:21.139261+00:00"
        }
      },
      "execution_count": null,
      "source": "! pip install pmdarima",
      "outputs": []
    },
    {
      "id": "37ac8eb3-f190-4970-b626-1d14f2458d31",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "2e6f8d0a-1b2f-4e5e-b270-e0790a79dad2"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:11:30.312128+00:00",
          "start_time": "2023-08-27T17:09:26.237783+00:00"
        }
      },
      "execution_count": null,
      "source": "import pmdarima as pm\nmodels = {}\nfor pond_name in o2_series.keys():\n    data = o2_boxcox[pond_name]['data']\n\n    # Using auto_arima to determine best parameters\n    model = pm.auto_arima(data,\n                          start_p=sarima_params[pond_name][0][0], \n                          start_q=sarima_params[pond_name][0][2], \n                          start_P=sarima_params[pond_name][1][0], \n                          start_Q=sarima_params[pond_name][1][2],\n                          D=1,\n                          m=2,\n                          seasonal=True, \n                          trace=True, \n                          error_action='ignore', \n                          suppress_warnings=True,\n                          stepwise=True)\n    models[pond_name] = model",
      "outputs": []
    },
    {
      "id": "29ce8a5a-6714-404e-863f-1f19e364082c",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "7d06fe7a-61a6-45ad-a504-a0e33da06ff4"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:11:30.496803+00:00",
          "start_time": "2023-08-27T17:11:30.337834+00:00"
        }
      },
      "execution_count": null,
      "source": "for pond_name in models.keys():\n    model = models[pond_name]\n\n    # Extracting parameters from the model\n    order = model.order\n    seasonal_order = model.seasonal_order\n    sarima_params[pond_name] = [order, seasonal_order]\n\n    print(f\"For pond {pond_name}:\")\n    print(f\"Starting parameters: ARIMA{str(sarima_params[pond_name][0])}\"\n          f\"x{str(sarima_params[pond_name][1])}\")\n    print(f\"Suggested by auto_arima: ARIMA{str(order)}x{str(seasonal_order)}\")\n    print('-' * 40)",
      "outputs": []
    },
    {
      "id": "f33d61e7-3689-4ae1-8188-ce577a3f3e7e",
      "cell_type": "markdown",
      "source": "### Analysing model perforamnce\n\nThis code will show the ACF and PACF plots for the residuals of each pond, giving insights into the remaining autocorrelations in the residuals. If the model is fitting well, most of the bars in the ACF and PACF plots should be within the blue confidence intervals, indicating that the residuals are white noise.\n\nAs we can see, residuals still show signs of autocorrelation, so I have to consider either adding external regressors (exogenous variables) to the SARIMA model or exploring other time series techniques.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "ceb26c1e-beee-431e-b74f-2c3c867ec81e",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "af180bb1-bd39-4991-8882-5b3cb35c103f"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:11:35.344229+00:00",
          "start_time": "2023-08-27T17:11:30.542421+00:00"
        }
      },
      "execution_count": null,
      "source": "pond_names = list(o2_series.keys())\nn_ponds = len(pond_names)\n\n# Adjusting the vertical size based on number of ponds\nfig, axes = plt.subplots(n_ponds, 2, figsize=(15, 4*n_ponds))  \nfig.suptitle('Residual Analysis')\n\nfor idx, pond_name in enumerate(pond_names):\n    model = models[pond_name]\n    residuals = model.resid()\n    # ACF plot for residuals of current pond\n    sm.graphics.tsa.plot_acf(residuals, lags=25, ax=axes[idx][0])\n    axes[idx][0].set_title(f'Residual ACF for {pond_name}')\n    axes[idx][0].grid(True, linestyle='--', alpha=0.5)\n    \n    # PACF plot for residuals of current pond\n    sm.graphics.tsa.plot_pacf(residuals, lags=25, ax=axes[idx][1])\n    axes[idx][1].set_title(f'Residual PACF for {pond_name}')\n    axes[idx][1].grid(True, linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.95, hspace=0.4)\nplt.show()",
      "outputs": []
    },
    {
      "id": "9f1c3050-f011-492e-9a4b-7cdf4937ded4",
      "cell_type": "markdown",
      "source": "### Ljung-Box Test on Residuals\nThe Ljung-Box test is applied to determine the presence of autocorrelation in a time series, which makes it especially useful when evaluating residuals following modeling. \n\nResiduals ideally should exhibit characteristics of white noise, meaning they should be independently and identically distributed. When the Ljung-Box test is applied to residuals, it aids in validating this white noise assumption. If there's significant autocorrelation found in the residuals, it could indicate that certain patterns in the data were not captured by the model, suggesting potential areas of improvement. \n\nConversely, residuals without detectable autocorrelation provide assurance that the model has effectively encapsulated the major underlying dynamics of the data, hinting at the model's predictive robustness.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "31e6759b-19db-4749-9cac-08882a3cb39f",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "edcd2efd-629e-42a7-809b-13d8bfdaafb6"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:11:36.345846+00:00",
          "start_time": "2023-08-27T17:11:36.145269+00:00"
        }
      },
      "execution_count": null,
      "source": "from statsmodels.stats.diagnostic import acorr_ljungbox\n\n# Create an empty DataFrame with desired columns\nresults_df = pd.DataFrame(columns=[\"Ljung-Box Test\", \"P-value\", \"Residual Count\", \"Residual Mean\", \"Residual Std\"])\n\nfor pond_name in o2_series.keys():\n    model = models[pond_name]\n    \n    # Extracting residuals\n    residuals = model.resid()\n\n    # Ljung-Box test\n    lb_test_df = acorr_ljungbox(residuals, lags=[10], return_df=True)\n    \n    # Descriptive statistics for residuals\n    res_desc = residuals.describe()\n    \n    # Append results to DataFrame, formatted as desired\n    results_df.loc[pond_name] = [\n        f\"{lb_test_df['lb_stat'].iloc[0]:.3f}\",\n        f\"{lb_test_df['lb_pvalue'].iloc[0]:.3f}\",\n        int(res_desc['count']),\n        f\"{res_desc['mean']:.3f}\",\n        f\"{res_desc['std']:.3f}\"\n    ]\n\n# Display the DataFrame\nprint(results_df)",
      "outputs": []
    },
    {
      "id": "6471dc32-5b8d-4d37-96ca-98750311614a",
      "cell_type": "markdown",
      "source": "### Summary of Model Performance and Initial Data Normality on Ponds\n\n#### np1\nInitial Normality: Moderate (JB: 21.7, p-value: 0.00)\nStationarity: No unit roots, stationary (ADF: -3.4, p-value: 0.01)\nLjung-Box Test: Some potential autocorrelation (P-value: 0.0155)\nAssessment:\nInitial data for np1 wasn't particularly normal but was stationary. After modeling, potential autocorrelation in residuals was observed.\n\n#### np2\nInitial Normality: High (JB: 1.2, p-value: 0.55)\nStationarity: No unit roots, stationary (ADF: -3.0, p-value: 0.04)\nLjung-Box Test: Residuals are white noise (P-value: 0.612)\nAssessment:\nnp2 had relatively normal initial data that was stationary. The model performed well with residuals resembling white noise.\n\n#### vp1\nInitial Normality: High (JB: 2.1, p-value: 0.35)\nStationarity: Unit roots, unstationary (ADF: -2.1, p-value: 0.23)\nLjung-Box Test: Residuals are white noise (P-value: 0.9351)\nAssessment:\nInitial data for vp1 was fairly normal but not stationary. However, the model showed good performance with residuals appearing as white noise.\n\n#### vp2\nInitial Normality: Moderate (JB: 10.4, p-value: 0.01)\nStationarity: No unit roots, stationary (ADF: -4.3, p-value: 0.00)\nLjung-Box Test: Residuals might be white noise (P-value: 0.3307)\nAssessment:\nvp2's initial data had some deviations from normality but was stationary. Post modeling, the residuals lean towards white noise.\n\n#### vp3\nInitial Normality: Low (JB: 19.9, p-value: 0.00)\nStationarity: No unit roots, stationary (ADF: -4.4, p-value: 0.00)\nLjung-Box Test: Some potential autocorrelation (P-value: 0.0006)\nAssessment:\nvp3 had less normal initial data, but it was stationary. The model's residuals indicate potential autocorrelation, suggesting suboptimal performance.\n\n#### vp4\nInitial Normality: Low (JB: 18.8, p-value: 0.00)\nStationarity: Unit roots, unstationary (ADF: -2.7, p-value: 0.07)\nLjung-Box Test: Residuals might be white noise (P-value: 0.1572)\nAssessment:\nDespite vp4's initial data not being very normal and being non-stationary, the model's residuals seem to be close to white noise.\n\n#### Overall Insights:\nPonds with High Initial Normality and Stationarity (np2): This pond showed good model performance with residuals resembling white noise.\n\nPonds with Moderate to Low Initial Normality but Stationary (np1, vp2, vp3): The correlation between initial data normality and model performance is mixed. While some (vp2) managed decent results, others (np1, vp3) showed potential issues like autocorrelation.\n\nPonds with High Initial Normality but Non-stationary (vp1, vp4): Despite having non-stationary data, these ponds' models managed decent results.\n\nIn conclusion, while there's a trend that ponds with initially more normal and stationary data tend to have models with better residuals, it's not strictly the case for all ponds. Stationarity, in conjunction with normality, plays a significant role in model performance.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "f5c1839c-7513-4418-9f4e-f02276babe58",
      "cell_type": "markdown",
      "source": "### Rolling-window validation\n\nThe below code block uses a rolling-window validation technique to forecast time series data for different ponds. At its core, this method involves training a model on a fixed-size window of historical data and then predicting the next value. After the prediction is made, the window moves one step forward in the time series, and the model is retrained to predict the next value. This process rolls on until predictions are generated for the entire series. Such an approach offers a dynamic way to assess how well the model might perform in a real-world scenario where it's constantly updated with new observations. The code also considers Box-Cox transformations, ensuring predictions are on the original data scale, and measures forecast accuracy using the Root Mean Squared Error (RMSE).",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "66595f02-a614-4074-99c3-8fdb7fe939ee",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "d4a2461b-30e5-4a77-88c0-80c4783e92a9"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:19:19.590883+00:00",
          "start_time": "2023-08-27T17:11:36.835958+00:00"
        }
      },
      "execution_count": null,
      "source": "from scipy.special import inv_boxcox\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n    \npreds = {}\nwindow = 30\n\nfor pond_name in o2_series.keys():\n    ser = o2_series[pond_name]\n    sert = o2_boxcox[pond_name]['data']\n    lambda_best_fit = o2_boxcox[pond_name]['lambda']\n    \n    # Rolling window validation\n    predictions = []\n\n    test = list(ser[window:])\n\n    for i in range(len(sert) - window + 1):\n        train_temp = sert[i: i + window]\n        # Will use SARIMAX from statsmodels for a change\n        model = SARIMAX(train_temp,\n                        order=sarima_params[pond_name][0],\n                        seasonal_order=sarima_params[pond_name][1])\n        model_fit = model.fit(disp=-1, maxiter=400)\n        yhat = model_fit.forecast(steps=1).iloc[0]\n        yhat_inv = inv_boxcox(yhat, lambda_best_fit)\n        predictions.append(yhat_inv)\n\n    # Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(test, predictions[:-1]))\n    r2 = r2_score(test, predictions[:-1])\n    print(f'pond: {pond_name}, RMSE: {rmse:.3f}, R2: {r2:.2f}')\n    \n    preds[pond_name] = [None]*(window) + predictions",
      "outputs": []
    },
    {
      "id": "2cfd49c9-7e7c-4e24-8e98-a814adb2ee66",
      "cell_type": "markdown",
      "source": "### Model Insights with Rolling Window Results:\n#### np1\nInitial State: Moderate normality and stationary.\n\nLjung-Box Test: Potential autocorrelation.\n\nRolling Window RMSE: 1.744, R2: 0.26 \n\nAssessment: Initial data for np1 had deviations from normality but was stationary. Model performance, based on RMSE, suggests potential room for improvement.\n#### np2\nInitial State: High normality and stationary.\n\nLjung-Box Test: Residuals are white noise.\n\nRolling Window RMSE: 1.512, R2: 0.12\n\nAssessment: The model for np2, which had relatively normal initial data, showed a relatively good performance based on its RMSE. However, R2 implies that the model explains only 12% of the data.\n#### vp1\nInitial State: High normality but non-stationary.\n\nLjung-Box Test: Residuals are white noise.\n\nRolling Window RMSE: 2.091, R2: 0.56\nThe warning message suggests that the optimization procedure for MLE did not converge. The model couldn't find a set of parameters that best fit the data within the specified number of iterations. The parameter estimates for your model might not be reliable.\n\nAssessment: The model's RMSE for vp1 is higher compared to np1 and np2, indicating potential model inefficiencies despite the residuals appearing as white noise and R2 is as high as 0.56.\n#### vp2\nInitial State: Moderate normality and stationary.\n\nLjung-Box Test: Residuals might be white noise.\n\nRolling Window RMSE: 1.972, R2: 0.40\n\nAssessment: vp2's model exhibited acceptible performance, indicated by its RMSE, despite its initial data showing some deviations from normality.\n#### vp3\nInitial State: Low normality but stationary.\n\nLjung-Box Test: Some potential autocorrelation.\n\nRolling Window RMSE: 0.779, R2: 0.75\n\nAssessment: Despite having less normal initial data, vp3's model showed good performance based on its low RMSE and high R2, although the residuals indicate potential autocorrelation.\n#### vp4\nInitial State: Low normality and non-stationary.\n\nLjung-Box Test: Residuals might be white noise.\n\nRolling Window RMSE: 0.992, R2: 0.74\n\nAssessment: vp4's model, with a decent RMSE and R2 score, demonstrates the model's ability to capture the dynamics of the time series data, even with less normal and non-stationary initial conditions.\n\n#### Overall Insights:\nThere's a discernible relationship between the initial conditions of the data (normality and stationarity) and the model's performance as assessed by RMSE. Generally, ponds with initially more normal and stationary data tend to have models with better residuals, but the rolling window technique does introduce some nuances to this observation. Specifically, some ponds, despite having less than ideal initial conditions, manage to produce models with decent RMSE values. On the flip side, certain ponds with good initial conditions might have higher RMSE values, suggesting potential model inefficiencies or challenges in predicting more complex patterns in the data.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "19fab09e-cc93-4d0f-b9f4-92d667edbb49",
      "cell_type": "markdown",
      "source": "### Predicted (red) vs actual O2 data (blue)",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "4ec8960d-a83f-449c-bf79-ff576e078bca",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "f0968b3a-f87e-40c9-be7f-1eda37cc38ec"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:19:21.860670+00:00",
          "start_time": "2023-08-27T17:19:19.640035+00:00"
        },
        "tags": [
          "show_line_numbers"
        ]
      },
      "execution_count": null,
      "source": "# Creating a 2x3 grid for the ponds (2 rows, 3 columns)\nfig, axes = plt.subplots(3, 2, figsize=(15, 10))\nfig.suptitle(\"Walk-forward validation: Predicted vs Actual values\")\n\n# Flatten the axes array for easier indexing\naxes = axes.ravel()\n\nfor idx, pond_name in enumerate(pond_names):\n    # Plotting the O2 values for each pond\n    axes[idx].plot(o2_series[pond_name], label=f'{pond_name} Actual values')\n    axes[idx].plot(preds[pond_name], color='red', label=\"Predicted values\")\n    axes[idx].set_title(pond_name)\n    axes[idx].set_xlabel('Entry index')\n    axes[idx].set_ylabel('O2')\n    axes[idx].legend()\n    axes[idx].grid(True, linestyle='--', alpha=0.5)\n\n# Adjust the layout so that plots do not overlap\nplt.tight_layout()\nplt.subplots_adjust(top=0.90)  # Adjust the top spacing so that the main title doesn't overlap\nplt.show()",
      "outputs": []
    },
    {
      "id": "bf8ccf1a-e78b-420c-a0ab-0813d70a2af6",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "python",
          "output_collection_id": "ceeb57bd-76b0-475a-a70d-736e3a1f6ca3"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T16:51:45.596828+00:00",
          "start_time": "2023-08-27T16:51:45.436368+00:00"
        }
      },
      "execution_count": null,
      "source": "",
      "outputs": []
    },
    {
      "id": "b2e25e8a-dab9-4e6a-a625-f5cde3e74b8c",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "406bef7b-7779-4654-9a9a-bfa3c776026f"
        },
        "ExecuteTime": {
          "end_time": "2023-08-27T17:27:36.929311+00:00",
          "start_time": "2023-08-27T17:27:36.752583+00:00"
        }
      },
      "execution_count": null,
      "source": "real_df = pd.DataFrame()\npred_df = pd.DataFrame()\n\nmax_length = 150  \n\nfor pond in pond_names:\n    actual_len = len(o2_series[pond])\n    preds_len = len(preds[pond][:-1])  \n    \n    # If the length is less than the maximum, prepend with NaNs\n    if actual_len < max_length:\n        nan_pad = pd.Series(data = ([np.nan] * (max_length - actual_len)), name='o2')\n        real_df[pond] = pd.concat([nan_pad, o2_series[pond]], ignore_index = True)\n        pred_df[pond] = pd.concat(\n            [nan_pad, pd.Series(data =preds[pond][:-1], name='o2')],\n            ignore_index = True\n        )\n    else:\n        real_df[pond] = o2_series[pond]\n        pred_df[pond] = preds[pond][:-1]\n\nreal_df.to_csv('SARIMA_data')\npred_df.to_csv('SARIMA_predictions')",
      "outputs": []
    },
    {
      "id": "deb12d74-1a93-4c06-907d-66138fb4203e",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "",
      "outputs": []
    }
  ]
}